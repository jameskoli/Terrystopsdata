{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing liblaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"C:\\\\Users\\\\HP\\\\Desktop\\\\terry dtops data\\\\Terry_Stops_20240301.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating over columns to identify categorical variables\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        unique_values = df[column].unique()\n",
    "        print(f\"Unique values for {column}: {unique_values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for null values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for NaN values in the DataFrame\n",
    "nan_values = df.isna().sum()\n",
    "\n",
    "# Display columns with NaN values and their respective counts\n",
    "print(\"Columns with NaN values:\")\n",
    "print(nan_values[nan_values > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating a new feature indicating whether a weapon was involved in the stop\n",
    "df['Weapon Involved'] = df['Weapon Type'].notnull().astype(int)\n",
    "\n",
    "\n",
    "# Creating a new feature representing the geographical area of the stop by combining \"Precinct\", \"Sector\", and \"Beat\"\n",
    "df['Geographical Area'] = df['Precinct'] + '-' + df['Sector'] + '-' + df['Beat']\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#interaction terms between officers race and call type\n",
    "df['Race_CallType'] = df['Officer Race'] + '_' + df['Call Type']\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the relevant collumns for my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['Officer Race', 'Subject Perceived Race', 'Stop Resolution', 'Geographical Area','Weapon Involved']\n",
    "race_relationship_data = df[selected_columns]\n",
    "\n",
    "race_relationship_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_relationship_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race composition for officers and subject involved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_counts = race_relationship_data['Officer Race'].value_counts()\n",
    "\n",
    "# Plotting the race composition\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=race_counts.index, y=race_counts.values, palette='viridis')\n",
    "plt.title('Race Composition of Officers')\n",
    "plt.xlabel('Officer Race')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.tight_layout()  \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of subject perceived race with rotated labels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Subject Perceived Race', data=race_relationship_data, palette='viridis')\n",
    "plt.title('Distribution of Subject Perceived Race')\n",
    "plt.xlabel('Subject Perceived Race')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90)  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between office and subject by race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# i'm Creating a pivot table to count occurrences of each combination of races\n",
    "pivot_table = race_relationship_data.pivot_table(index='Officer Race', columns='Subject Perceived Race', aggfunc='size', fill_value=0)\n",
    "\n",
    "# Plotting the heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, fmt='d', cmap='viridis')\n",
    "plt.title('Relationship between Officer Race and Subject Perceived Race')\n",
    "plt.xlabel('Subject Perceived Race')\n",
    "plt.ylabel('Officer Race')\n",
    "plt.xticks(rotation=45, ha='right')  \n",
    "plt.yticks(rotation=0, va='center')  \n",
    "plt.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Testing: Relationship Between Officer's Race and Stop Resolution\n",
    "\n",
    "### Hypotheses:\n",
    "- **Null Hypothesis (H0):** There is no association between the officer's race and the stop resolution.\n",
    "- **Alternative Hypothesis (H1):** There is an association between the officer's race and the stop resolution.\n",
    "\n",
    "### Finding:\n",
    "Based on the small p-value obtained from the chi-square test (much smaller than the significance level of 0.05), we reject the null hypothesis in favor of the alternative hypothesis, indicating that there is indeed a significant association between the officer's race and the stop resolution.\n",
    "This analysis suggests that the type of stop resolution is not independent of the officer's race, indicating potential biases or underlying factors influencing the stop resolution based on the officer's race."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Creating a contingency table\n",
    "contingency_table = pd.crosstab(race_relationship_data['Officer Race'], race_relationship_data['Stop Resolution'])\n",
    "\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Calculating chi-square statistic and p-value\n",
    "chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "print(\"\\nChi-square Statistic:\", chi2)\n",
    "print(\"P-value:\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contingency table visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the contingency table\n",
    "contingency_table = [[79, 0, 148, 113, 5],\n",
    "                     [667, 8, 1368, 666, 28],\n",
    "                     [494, 10, 1240, 610, 23],\n",
    "                     [927, 19, 2212, 644, 33],\n",
    "                     [111, 2, 239, 175, 5],\n",
    "                     [629, 16, 1278, 755, 33],\n",
    "                     [998, 18, 2039, 843, 37],\n",
    "                     [21, 1, 69, 9, 0],\n",
    "                     [10519, 140, 20695, 10678, 562]]\n",
    "\n",
    "# Defining officer races and stop resolutions\n",
    "officer_races = ['American Indian/Alaska Native', 'Asian', 'Black or African American', \n",
    "                 'Hispanic or Latino', 'Nat Hawaiian/Oth Pac Islander', 'Not Specified', \n",
    "                 'Two or More Races', 'Unknown', 'White']\n",
    "stop_resolutions = ['Arrest', 'Citation / Infraction', 'Field Contact', \n",
    "                    'Offense Report', 'Referred for Prosecution']\n",
    "\n",
    "# Creating a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(contingency_table, annot=True, cmap='Blues', fmt='d',\n",
    "            xticklabels=stop_resolutions, yticklabels=officer_races, cbar=True)\n",
    "plt.title('Contingency Table: Officer Race vs Stop Resolution')\n",
    "plt.xlabel('Stop Resolution')\n",
    "plt.ylabel('Officer Race')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop resolutions in randomly selected areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select random areas\n",
    "random_areas = df['Geographical Area'].sample(n=11, random_state=42)  # Change n to the number of random areas you want\n",
    "\n",
    "# Filter DataFrame to include only randomly selected areas\n",
    "df_random_areas = df[df['Geographical Area'].isin(random_areas)]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_random_areas, x='Geographical Area', hue='Stop Resolution')\n",
    "plt.title('Stop Resolutions in Randomly Selected Geographical Areas')\n",
    "plt.xlabel('Geographical Area')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Stop Resolution')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop resolutions with weapons involvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Stop Resolution', hue='Weapon Involved')\n",
    "plt.title('Stop Resolutions with Weapon Involvement')\n",
    "plt.xlabel('Stop Resolution')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Weapon Involved', labels=['No', 'Yes'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contingency_table = pd.crosstab(race_relationship_data['Stop Resolution'], [race_relationship_data['Officer Race'], race_relationship_data['Subject Perceived Race'], race_relationship_data['Geographical Area']])\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "# Display results\n",
    "print(\"Contingency Table:\")\n",
    "print(contingency_table)\n",
    "print(\"\\nChi-square Statistic:\", chi2)\n",
    "print(\"P-value:\", p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# contingency table data\n",
    "contingency_table = {\n",
    "    'Stop Resolution': ['Arrest', 'Citation / Infraction', 'Field Contact', 'Offense Report', 'Referred for Prosecution'],\n",
    "    'American Indian/Alaska Native': [79, 0, 148, 113, 5],\n",
    "    'Asian': [667, 8, 1368, 666, 28],\n",
    "    'Black or African American': [494, 10, 1240, 610, 23],\n",
    "    'Hispanic or Latino': [927, 19, 2212, 644, 33],\n",
    "    'Nat Hawaiian/Oth Pac Islander': [111, 2, 239, 175, 5],\n",
    "    'Not Specified': [629, 16, 1278, 755, 33],\n",
    "    'Two or More Races': [998, 18, 2039, 843, 37],\n",
    "    'Unknown': [21, 1, 69, 9, 0],\n",
    "    'White': [10519, 140, 20695, 10678, 562]\n",
    "}\n",
    "\n",
    "# Creating DataFrame\n",
    "df_contingency = pd.DataFrame(contingency_table)\n",
    "df_contingency.set_index('Stop Resolution', inplace=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "df_contingency.plot(kind='bar', stacked=True, figsize=(12,8))\n",
    "plt.title('Stop Resolutions by Officer Race')\n",
    "plt.xlabel('Stop Resolution')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Officer Race')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing one-hot encoding for categorical variables\n",
    "categorical_cols = ['Officer Race', 'Subject Perceived Race', 'Geographical Area']\n",
    "data_encoded = pd.get_dummies(race_relationship_data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Splitting the data into features and target variable\n",
    "X = data_encoded.drop('Stop Resolution', axis=1)\n",
    "y = data_encoded['Stop Resolution']\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELLING AND MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Instantiating logistic regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# predictions on the training set\n",
    "train_preds = logistic_model.predict(X_train)\n",
    "\n",
    "# predictions on the testing set\n",
    "test_preds = logistic_model.predict(X_test)\n",
    "\n",
    "# model evaluation\n",
    "train_accuracy = accuracy_score(y_train, train_preds)\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# The Classifications report/result\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, test_preds, zero_division=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# model dicts.\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Training the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on the test set\n",
    "    test_preds = model.predict(X_test)\n",
    "    \n",
    "    # model evaluation\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    \n",
    "    # Classification report\n",
    "    report = classification_report(y_test, test_preds)\n",
    "    \n",
    "    print(f\"Model: {name}\")\n",
    "    print(f\"Testing Accuracy: {test_accuracy}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROSS-VALIDATION\n",
    "performing cross-validation to ensure that the model performance estimates are robust and not overly optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest has the highest mean cross-validation score, indicating better performance on average compared to the other models. Additionally, Random Forest has the lowest standard deviation among the models, suggesting more consistent performance across different folds. Therefore, based on these cross-validation results, Random Forest appears to be the best model among the ones tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "# models for cross validation\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
    "}\n",
    "\n",
    "# Performing cross-validation for each model\n",
    "for name, model in models.items():\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5)\n",
    "    print(f\"Model: {name}\")\n",
    "    print(\"Cross-validation scores:\", cv_scores)\n",
    "    print(\"Mean CV score:\", cv_scores.mean())\n",
    "    print(\"Standard deviation of CV scores:\", cv_scores.std())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(RandomForestClassifier(), X_train, y_train, cv=5)\n",
    "\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training accuracy')\n",
    "plt.plot(train_sizes, test_scores_mean, label='Testing accuracy')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning curves')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I selected random forest as my best model and performed hyperparameter tuning on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Set random seed for Python\n",
    "random.seed(42)\n",
    "\n",
    "# Set random seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the hyperparameter distributions\n",
    "param_dist = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'min_samples_leaf': randint(1, 20),\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "# Instantiate the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=50, cv=5, scoring='accuracy', random_state=42)\n",
    "\n",
    "# Fit the random search to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Model:\", best_rf)\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Cross-Validation Score:\", best_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_preds = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "precision = precision_score(y_test, test_preds, average='weighted')\n",
    "recall = recall_score(y_test, test_preds, average='weighted')\n",
    "f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, test_preds)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STORING MY HYPERPARAMETER VALUES SO THAT I DONT RUN THEM EVERYTIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Assuming you have your trained model stored in 'best_rf' and your test data stored in 'X_test' and 'y_test'\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify misclassified instances\n",
    "misclassified_indices = y_test != test_preds\n",
    "misclassified_instances = X_test[misclassified_indices]\n",
    "true_labels = y_test[misclassified_indices]\n",
    "predicted_labels = test_preds[misclassified_indices]\n",
    "\n",
    "# Examine features of misclassified instances\n",
    "print(\"Misclassified Instances:\")\n",
    "print(misclassified_instances)\n",
    "\n",
    "# Visualize data (example: histogram of a feature)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize data (example: histogram of a feature)\n",
    "plt.hist(misclassified_instances.iloc[:, 0])  # Replace 0 with the index of the feature you want to visualize\n",
    "plt.xlabel(\"Feature Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Histogram of Misclassified Feature\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL COMPARISON VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining and training the Decision Tree model\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Defining and training the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(random_state=42)\n",
    "random_forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Defining and training the AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Defining and training the Gradient Boosting model\n",
    "gradient_boosting_model = GradientBoostingClassifier(random_state=42)\n",
    "gradient_boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Defining and training the Naive Bayes model\n",
    "naive_bayes_model = GaussianNB()\n",
    "naive_bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have imported necessary libraries and defined `get_metrics()` function\n",
    "\n",
    "best_rf_report = get_metrics(best_rf, X_test, y_test)\n",
    "\n",
    "# Getting metrics for each model\n",
    "models = [decision_tree_model, random_forest_model, adaboost_model, gradient_boosting_model, naive_bayes_model]\n",
    "\n",
    "for model in models:\n",
    "    report = get_metrics(model, X_test, y_test)\n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    print()\n",
    "\n",
    "# Print hyper-tuned model's report\n",
    "print(\"Hyper-Tuned Random Forest Model:\")\n",
    "print(\"Classification Report:\")\n",
    "print(best_rf_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the hyper-tuned model's report\n",
    "best_rf_report = get_metrics(best_rf, X_test, y_test)\n",
    "\n",
    "# Add the hyper-tuned model's report to the dictionary\n",
    "reports['Hyper-Tuned Random Forest'] = best_rf_report\n",
    "\n",
    "# Extracting metrics\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for model_name, report in reports.items():\n",
    "    precision, recall, f1_score = extract_metrics(report)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1_score)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Plotting precision\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.bar(model_names, precisions, color='blue')\n",
    "plt.title('Precision')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Plotting recall\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.bar(model_names, recalls, color='green')\n",
    "plt.title('Recall')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Recall')\n",
    "\n",
    "# Plotting F1-score\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.bar(model_names, f1_scores, color='red')\n",
    "plt.title('F1-score')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('F1-score')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
